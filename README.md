Développement d’un système de Data Management et d’intégration de données pour l’optimisation de la recherche d’emploi en France :
Utilisation de Python, SQLite et PySpark pour collecter, transformer et interroger des données multi-sources 
(offres d’emploi, transports, événements professionnels, coût de la vie).
Mise en place de wrappers (adaptateurs) pour l’extraction de données hétérogènes (API, JSON, CSV), 
transformation et chargement dans une base SQLite (version locale) puis dans Databricks via PySpark (version distribuée).
Implémentation d’un médiateur (mediator) pour interroger dynamiquement les sources intégrées à travers des vues SQL et Spark SQL unifiées, 
permettant de croiser critères géographiques, économiques et sociaux afin de recommander les opportunités les plus adaptées
